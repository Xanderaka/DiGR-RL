This is the code repository for Xander Vreeswijk's Master Thesis.

Preparing the dataset
---------------------

Since this approach is based on the DARLInG approach made by Yvan Satyawan, preparing the dataset is done in the same way as it is done in the DARLInG README.

   1. Generate the small dataset, if desired.
      1. Generate an index for the small dataset using `src/data_utils/generate_dataset_index.py`.
         1. To generate the single-user leave out, run with `generate_dataset_index.py [PATH_TO_DATA_ROOT] -u`.
         2. Use `-s` instead of `-u` to make a single-domain index.
         3. Use `-r` to make an index for two rooms, one for training and one for validation.
         4. Add `-n 3` to only use 3 repetitions instead of the full dataset. Useful for debugging as it means a smaller dataset.
      2. Generate the smaller datasets using `src/data_utils/generate_smaller_splits.py`.
         1. To generate the single-user split, run with `generate_smaller_splits.py [PATH_TO_DATA_ROOT] single_user`.
         2. It is possible to replace `single_user` with whatever index file suffix was generated by `generate_dataset_index.py`.
   2. Otherwise, generate the dataset index using `src/data_utils/generate_dataset_index.py`. This will generate the index of the full dataset.
   3. Calculate the mean and standard deviation of amplitude and phase using `src/data_utils/calculate_mean_std.py`
      1. If not using the full dataset, copy both the indexes and generated mean_std.csv into the smaller split directory
   4. Pregenerate transformations, as they take too long to generate during training, using `src/data_utils/pregenerate_transform.py`
      1. Run using the config file for a given experiment with `pregenerate_transform.py [PATH_TO_CONFIG_FILE]`.

Config File
-----------

The config files are inside run_configs directory which contains the experimental runs configurations used for the experiments.
utils/config_parser.py contain the full list of possible configuration parameters.

Training the model
------------------

The approach is trained by experiments/train_runner.py.
It is run by using train_runner.py [PATH_TO_CONFIG_FILE].

Hyperparameter tuning was performed using the Optuna framework, an efficient and automated optimization tool designed to find the best set of hyperparameters for a given model. Optuna conducts multiple trial runs, exploring different configurations, and evaluates their performance based on a defined objective function. After completing these runs, it identifies and returns the optimal combination of hyperparameters that yielded the best results, thus enhancing the modelâ€™s overall accuracy and generalization capabilities.

All the other files
-------------------

All the other files and scripts are carefully documented with Python Docstrings to indicate what they do and how they work.
train_runner.py is for general model training, train.py hosts the main training loop code.
All other scripts in experiments/ were used to obtain different information necessary for finishing the Master Thesis.